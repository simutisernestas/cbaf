https://github.com/binli826/LSTM-Autoencoders
https://towardsdatascience.com/autoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e
https://github.com/tejaslodaya/timeseries-clustering-vae
https://github.com/jjakimoto/DQN
https://github.com/openai/baselines/tree/master/baselines/ppo2 

# COMMANDS:
python -m baselines.run --alg=ppo2 --env=gym_portfolio:asset-v0 --num_timesteps=2e7 --save_path=~/models/asset_20M_ppo2 --log_path=~/logs/Asset/
python -m baselines.run --alg=ppo2 --env=gym_portfolio:asset-v0 --num_timesteps=1e7 --load_path=~/models/asset_20M_ppo2 --log_path=~/logs/Asset/
python -m baselines.run --alg=ppo2 --env=gym_portfolio:asset-v0 --num_timesteps=0   --load_path=~/models/asset_20M_ppo2 --play

python train.py --algo ppo --env gym_portfolio:asset-v0 -n 50000 -optimize --n-trials 1000 --n-jobs 4 --sampler tpe --pruner median

# DONE:
 * sample more sequences by random start indexes
 * reduce low-high & open-close columns into single by subtracting them
 * try removing kl-loss? - doesn't affect too much
 * episode ends when losses drop below a threshold ~5%
 * rework reward to be expressed as profit
 * make the encoded vector much much smaller (20*6 < 512)
 * compare previous high to next low
 * explore coinbase database for more data (THAT's all)
 * model naming by parameters used

# TODO:
 * binance has a lot more data, collect that
 * research RL approaches
 * research that DQN link
 * add live data to RL step function along side the encoded vector
 * validation set for AE
 * validation set for RL
 * extensive testing of RL environment
 * separating the future step might be a good idea
 * add current state in observation also
 * add fees feature on top of the buy/sell